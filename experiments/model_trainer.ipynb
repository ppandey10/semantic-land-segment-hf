{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy'] = 'http://proxy1.bgc-jena.mpg.de:3128' \n",
    "os.environ['https_proxy'] = 'http://proxy1.bgc-jena.mpg.de:3128'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FILES TO UPDATE: \n",
    "- config.yaml\n",
    "- params.yaml\n",
    "- entity\n",
    "- configuration manager in src config\n",
    "- components\n",
    "- pipeline\n",
    "- main.py\n",
    "- app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    preprocessed_data_path: Path\n",
    "    BACKBONE: str\n",
    "    encoder_weights: str\n",
    "    DEVICE: str\n",
    "    n_classes: int\n",
    "    epochs: int\n",
    "    activation: str\n",
    "    loss: str\n",
    "    optimizer: str\n",
    "    metrics: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the ConfigurationManager\n",
    "from landcover_segmentation.constants import *\n",
    "from landcover_segmentation.utils.common import create_directories, read_yaml\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILEPATH,\n",
    "        params_filepath = PARAMS_FILEPATH\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        # Create artifacts folder\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.SMPTrainingArguments\n",
    "\n",
    "        # Create directory\n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            preprocessed_data_path=config.preprocessed_data_path,\n",
    "            BACKBONE=params.BACKBONE,\n",
    "            encoder_weights=params.encoder_weights,\n",
    "            DEVICE=params.DEVICE,\n",
    "            n_classes=params.n_classes,\n",
    "            epochs=params.epochs,\n",
    "            activation=params.activation,\n",
    "            loss=params.loss,\n",
    "            optimizer=params.optimizer,\n",
    "            metrics=params.metrics\n",
    "        )\n",
    "\n",
    "        return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from landcover_segmentation.logging import logger\n",
    "from landcover_segmentation.pipeline.step_03_data_loader import DataLoaderPipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: ModelTrainerConfig\n",
    "    ):\n",
    "        self.config = config\n",
    "\n",
    "        self.data_loader_pipeline = DataLoaderPipeline()\n",
    "        self.generators = self.data_loader_pipeline.main()\n",
    "\n",
    "    def smp_model(self): # TODO: include other models as well\n",
    "        return smp.Unet(\n",
    "            encoder_name=self.config.BACKBONE,\n",
    "            encoder_weights=self.config.encoder_weights,\n",
    "            in_channels=3, classes=self.config.n_classes, \n",
    "            activation=self.config.activation\n",
    "        )\n",
    "    \n",
    "    def loss_fn(self, out, mask):\n",
    "        if self.config.loss == 'jaccard':\n",
    "            loss_fnc = smp.losses.JaccardLoss(mode=smp.losses.MULTILABEL_MODE)\n",
    "        \n",
    "        if self.config.loss == 'dice':\n",
    "            loss_fnc = smp.losses.DiceLoss(mode=smp.losses.MULTILABEL_MODE)\n",
    "        \n",
    "        return loss_fnc(out, mask)\n",
    "    \n",
    "    def metrics(self):\n",
    "        if self.config.metrics == 'iou':\n",
    "            metrics = [smp.metrics.iou_score] \n",
    "\n",
    "        return metrics\n",
    "    \n",
    "    def optimizer(self):\n",
    "        if self.config.optimizer == 'adam':\n",
    "            opt = optim.Adam(self.smp_model().parameters(), lr=0.001)\n",
    "\n",
    "        return opt\n",
    "    \n",
    "    def train_loader(self):\n",
    "        train_data_loader = self.generators['train']\n",
    "        try:\n",
    "            next(train_data_loader)\n",
    "        except StopIteration:\n",
    "            pass\n",
    "\n",
    "        return train_data_loader, self.data_loader_pipeline.data_loader._total_batches\n",
    "    \n",
    "    def val_loader(self):\n",
    "        val_data_loader = self.generators['val']\n",
    "        try:\n",
    "            next(val_data_loader)\n",
    "        except StopIteration:\n",
    "            pass\n",
    "\n",
    "        return val_data_loader, self.data_loader_pipeline.data_loader._total_batches\n",
    "    \n",
    "    def test_loader(self):\n",
    "        test_data_loader = self.generators['test']\n",
    "        try:\n",
    "            next(test_data_loader)\n",
    "        except StopIteration:\n",
    "            pass\n",
    "\n",
    "        return test_data_loader, self.data_loader_pipeline.data_loader._total_batches\n",
    "    \n",
    "    def model_trainer(\n",
    "        self,\n",
    "        device: str = None\n",
    "    ):\n",
    "        if device=='cpu':\n",
    "            self.config.DEVICE = device\n",
    "        \n",
    "        model = self.smp_model()\n",
    "        model.double().to(device)\n",
    "\n",
    "        train_dataloader, train_total_batch = self.train_loader()\n",
    "        val_dataloader, val_total_batch = self.val_loader()\n",
    "\n",
    "        # train model\n",
    "        num_epochs = self.config.epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "            with tqdm(total=train_total_batch, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as pbar:\n",
    "                for images, masks in train_dataloader:\n",
    "                    images = images.double().to(device)\n",
    "                    masks = masks.double().to(device)\n",
    "                    \n",
    "                    self.optimizer().zero_grad()\n",
    "                    outputs = model(images)\n",
    "                    loss = self.loss_fn(outputs, masks)\n",
    "                    loss.backward()\n",
    "                    self.optimizer().step()\n",
    "                    epoch_loss += loss.item()\n",
    "                    \n",
    "                    pbar.set_postfix({\"loss\": epoch_loss/train_total_batch})\n",
    "                    pbar.update(1)\n",
    "\n",
    "            logger.info(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/train_total_batch}\")\n",
    "\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with tqdm(total=val_total_batch, desc=f\"Validation {epoch+1}/{num_epochs}\", unit=\"batch\") as pbar:\n",
    "                with torch.no_grad():\n",
    "                    for images, masks in val_dataloader:\n",
    "                        images = images.to(device)\n",
    "                        masks = masks.to(device)\n",
    "                        outputs = model(images)\n",
    "                        loss = self.loss_fn(outputs, masks)\n",
    "                        val_loss += loss.item()\n",
    "                        \n",
    "                        pbar.set_postfix({\"val_loss\": val_loss/val_total_batch})\n",
    "                        pbar.update(1)\n",
    "\n",
    "            logger.info(f\"Validation Loss: {val_loss/val_total_batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write PIPELINE\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer = ModelTrainer(config=model_trainer_config)\n",
    "    model_trainer.model_trainer()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TextSum",
   "language": "python",
   "name": "texts"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
